# -*- coding: utf-8 -*-
"""my_twitter_crawler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHFJ6JRwJ4y7UhFxDlGLGHAeS4uAi3kX
"""

pip install hazm

pip install hezar

import pandas as pd
df = pd.read_csv('/content/88.csv')

# Print all columns
for column in df.columns:
    print(column)

import networkx as nx
import matplotlib.pyplot as plt
G = nx.DiGraph()

for index, row in df.iterrows():
    full_text = row['full_text']
    created_at = row['created_at']
    in_reply_to_screen_name = row['in_reply_to_screen_name']
    user_screen_name = row['user/screen_name']
    weight = row['user/followers_count']
    if G.has_node(user_screen_name) == False:
         G.add_node(user_screen_name , weight=weight , text = full_text , created = created_at)

    #add edge for each node
    # Check if reply_to_zero is NaN
    if pd.notnull(in_reply_to_screen_name):
       G.add_edge(user_screen_name ,in_reply_to_screen_name)

import networkx as nx
from nltk.probability import FreqDist
from hazm import *
# Aggregate texts from all nodes
all_texts = []
for index, row in df.iterrows():
    full_text = row['full_text']
    all_texts.append(full_text)
# Tokenize and calculate word frequencies for all texts
tokens = []
normalizer = Normalizer()
for text in all_texts:
    tokens.extend(word_tokenize(normalizer.normalize(text)))

fdist = FreqDist(tokens)

# Sort words by frequency in descending order
sorted_words = sorted(fdist.items(), key=lambda item: item[1], reverse=True)

# Print words and their frequencies
for word, frequency in sorted_words:
    print(word, ":", frequency)

# Calculate betweenness centrality for each node
betweenness_centrality = nx.betweenness_centrality(G)

# Sort the nodes based on their betweenness centrality values
sorted_nodes = sorted(betweenness_centrality, key=betweenness_centrality.get, reverse=True)

# Get the top 5 nodes with the highest centrality
top_nodes = sorted_nodes[:5]

print("Top 5 nodes with highest centrality:")
for node in top_nodes:
    print(f"Node: {node}, Centrality: {betweenness_centrality[node]}")

# Calculate degree centrality for each node
degree_centrality = nx.degree_centrality(G)

# Sort the nodes based on their degree centrality values
sorted_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)

# Get the top 5 nodes with the highest degree centrality
top_nodes = sorted_nodes[:5]

print("Top 5 nodes with highest degree centrality:")
for node in top_nodes:
    print(f"Node: {node}, Degree Centrality: {degree_centrality[node]}")

# Calculate closeness centrality for each node
closeness_centrality = nx.closeness_centrality(G)

# Sort the nodes based on their closeness centrality values
sorted_nodes = sorted(closeness_centrality, key=closeness_centrality.get, reverse=True)

# Get the top 5 nodes with the highest closeness centrality
top_nodes = sorted_nodes[:5]

print("Top 5 nodes with highest closeness centrality:")
for node in top_nodes:
    print(f"Node: {node}, Closeness Centrality: {closeness_centrality[node]}")

# Calculate eigenvector centrality for each node
eigenvector_centrality = nx.eigenvector_centrality(G)

# Sort the nodes based on their eigenvector centrality values
sorted_nodes = sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)

# Get the top 5 nodes with the highest eigenvector centrality
top_nodes = sorted_nodes[:5]

print("Top 5 nodes with highest eigenvector centrality:")
for node in top_nodes:
    print(f"Node: {node}, Eigenvector Centrality: {eigenvector_centrality[node]}")

import networkx as nx

# Calculate out-degrees of all nodes
out_degrees = G.out_degree()

# Sort nodes based on out-degrees in descending order
sorted_nodes = sorted(out_degrees, key=lambda x: x[1], reverse=True)

# Print the top 5 nodes with max out-degree
top_nodes = sorted_nodes[:5]
for node, out_degree in top_nodes:
    print(f"Node {node} has {out_degree} outgoing edges.")

# Select a single node
node = 'minoosadat13'


# Get the outbound nodes (successors)
outbound_nodes = list(G.successors(node))

# Get the inbound nodes (predecessors)
inbound_nodes = list(G.predecessors(node))

# Create the subgraph with the specified node and its outbound and inbound nodes
subgraph_nodes = [node] + outbound_nodes + inbound_nodes
subgraph = G.subgraph(subgraph_nodes)

# Draw the subgraph
nx.draw(subgraph, with_labels=True , font_size=10)

# Display the subgraph
plt.show()

communities = nx.algorithms.community.girvan_newman(G)

# Print the communities
for community in next(communities):
    print(f"Community: {community} ")

from hezar.models import Model
from hazm import *

model = Model.load("hezarai/bert-fa-sentiment-dksf")

# Aggregate texts from all nodes
normalizer = Normalizer()
for index, row in df.iterrows():
    full_text = row['full_text']
    outputs = model.predict(normalizer.normalize(full_text))
    print(f"{full_text}: {outputs} ")
    print("------------------------------------------------------------")

from hezar.models import Model
from hazm import *


model = Model.load("hezarai/bert-fa-sentiment-dksf")

# Aggregate texts from all nodes
all_texts = []
for index, row in df.iterrows():
    full_text = row['full_text']
    all_texts.append(full_text)
# Tokenize and calculate word frequencies for all texts
tokens = []
normalizer = Normalizer()
for text in all_texts:
    tokens.extend(word_tokenize(normalizer.normalize(text)))

fdist = FreqDist(tokens)

# Sort words by frequency in descending order
sorted_words = sorted(fdist.items(), key=lambda item: item[1], reverse=True)

# Print words and their frequencies
for word, frequency in sorted_words:
    print(word, ":", model.predict(word))